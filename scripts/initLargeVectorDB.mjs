#!/usr/bin/env node

import sqlite3 from 'sqlite3';
import { open } from 'sqlite';
import fs from 'fs';
import path from 'path';
import readline from 'readline';

const FASTTEXT_FILE = path.join(process.cwd(), 'data', 'fasttext', 'cc.ko.300.vec');
const LARGE_KOREAN_WORDS_FILE = path.join(process.cwd(), 'data', 'korean_words_large.txt');
const DB_PATH = path.join(process.cwd(), 'data', 'vectors.db');

// Direct VectorDB implementation for large-scale initialization
class LargeVectorDB {
  constructor() {
    this.db = null;
  }

  async initialize() {
    // Ensure data directory exists
    const dataDir = path.dirname(DB_PATH);
    if (!fs.existsSync(dataDir)) {
      fs.mkdirSync(dataDir, { recursive: true });
    }

    this.db = await open({
      filename: DB_PATH,
      driver: sqlite3.Database
    });

    // Drop existing table and recreate
    await this.db.exec(`DROP TABLE IF EXISTS word_vectors`);
    
    // ë²¡í„° ì €ì¥ í…Œì´ë¸” ìƒì„±
    await this.db.exec(`
      CREATE TABLE word_vectors (
        word TEXT PRIMARY KEY,
        vector BLOB NOT NULL,
        frequency INTEGER DEFAULT 0,
        created_at DATETIME DEFAULT CURRENT_TIMESTAMP
      )
    `);

    // ì¸ë±ìŠ¤ ìƒì„± (ë¹ ë¥¸ ê²€ìƒ‰ìš©)
    await this.db.exec(`
      CREATE INDEX idx_word_frequency ON word_vectors(frequency DESC);
    `);

    console.log('âœ… Large VectorDB initialized successfully');
  }

  async loadLargeKoreanWords(vecFilePath, targetWords) {
    if (!this.db) throw new Error('Database not initialized');

    const fileStream = fs.createReadStream(vecFilePath);
    const rl = readline.createInterface({ input: fileStream, crlfDelay: Infinity });

    let lineCount = 0;
    let insertedCount = 0;
    const batchSize = 2000;
    let batch = [];
    const targetWordsSet = new Set(targetWords); // ë¹ ë¥¸ ê²€ìƒ‰ì„ ìœ„í•œ Set

    console.log('ğŸ“¥ Loading large Korean FastText vectors...');
    console.log(`ğŸ¯ Target words: ${targetWords.length}`);

    for await (const line of rl) {
      if (lineCount === 0) {
        lineCount++;
        continue; // ì²« ë²ˆì§¸ ë¼ì¸ì€ ë©”íƒ€ë°ì´í„°
      }

      const parts = line.trim().split(' ');
      const word = parts[0];
      
      // í•œêµ­ì–´ ë‹¨ì–´ë§Œ í•„í„°ë§
      if (!this.isKoreanWord(word)) continue;
      
      // ëŒ€ìš©ëŸ‰ íƒ€ê²Ÿ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ í™•ì¸
      if (!targetWordsSet.has(word)) continue;

      const vector = parts.slice(1).map(x => parseFloat(x));
      if (vector.length !== 300) continue; // FastTextëŠ” 300ì°¨ì›

      // numpy arrayë¥¼ Bufferë¡œ ì§ë ¬í™”
      const vectorBuffer = this.serializeVector(vector);
      
      batch.push({ word, vectorBuffer });

      if (batch.length >= batchSize) {
        await this.insertBatch(batch);
        insertedCount += batch.length;
        batch = [];
        console.log(`ğŸ“Š Inserted ${insertedCount} vectors... (${(insertedCount/targetWords.length*100).toFixed(1)}%)`);
      }

      lineCount++;
    }

    // ë§ˆì§€ë§‰ ë°°ì¹˜ ì²˜ë¦¬
    if (batch.length > 0) {
      await this.insertBatch(batch);
      insertedCount += batch.length;
    }

    console.log(`âœ… Total vectors loaded: ${insertedCount}`);
    return insertedCount;
  }

  async insertBatch(batch) {
    if (!this.db) return;

    const stmt = await this.db.prepare(`
      INSERT OR REPLACE INTO word_vectors (word, vector) VALUES (?, ?)
    `);

    for (const { word, vectorBuffer } of batch) {
      await stmt.run(word, vectorBuffer);
    }

    await stmt.finalize();
  }

  serializeVector(vector) {
    // Float64Arrayë¡œ ë³€í™˜ í›„ Bufferë¡œ ì§ë ¬í™”
    const float64Array = new Float64Array(vector);
    return Buffer.from(float64Array.buffer);
  }

  isKoreanWord(word) {
    return /^[ê°€-í£]+$/.test(word);
  }

  async getWordVector(word) {
    if (!this.db) throw new Error('Database not initialized');

    const result = await this.db.get(
      'SELECT vector FROM word_vectors WHERE word = ?',
      word
    );

    if (!result) return null;
    
    // Bufferì—ì„œ Float64Arrayë¡œ ë³µì› í›„ ë°°ì—´ë¡œ ë³€í™˜
    const float64Array = new Float64Array(result.vector.buffer, result.vector.byteOffset, result.vector.length / 8);
    return Array.from(float64Array);
  }

  async close() {
    if (this.db) {
      await this.db.close();
      this.db = null;
    }
  }
}

async function initializeLargeDatabase() {
  console.log('ğŸš€ Starting LARGE VectorDB initialization...');
  
  const vectorDB = new LargeVectorDB();
  
  try {
    // 1. ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
    await vectorDB.initialize();
    console.log('âœ… Database initialized');

    // 2. ëŒ€ìš©ëŸ‰ í•œêµ­ì–´ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ (10,000ê°œ)
    let targetWords = [];
    
    if (fs.existsSync(LARGE_KOREAN_WORDS_FILE)) {
      const largeWords = fs.readFileSync(LARGE_KOREAN_WORDS_FILE, 'utf-8')
        .split('\n')
        .map(line => line.trim())
        .filter(word => word.length > 0);
      targetWords = largeWords;
      console.log(`ğŸ“š Loaded ${targetWords.length} large Korean words`);
    } else {
      console.log('âŒ Large Korean words file not found');
      process.exit(1);
    }

    // 3. FastText ë²¡í„° íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
    if (!fs.existsSync(FASTTEXT_FILE)) {
      console.log('âš ï¸  FastText file not found at:', FASTTEXT_FILE);
      console.log('ğŸ’¡ Run: npm run setup:fasttext first');
      process.exit(1);
    }

    // 4. ë²¡í„° ë°ì´í„°ë¥¼ DBì— ë¡œë“œ
    console.log('ğŸ“¥ Loading LARGE FastText vectors into database...');
    console.log('â³ This may take 10-15 minutes for 10,000 words...');
    
    const insertedCount = await vectorDB.loadLargeKoreanWords(FASTTEXT_FILE, targetWords);
    
    // 5. í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰
    console.log('ğŸ§ª Running test queries...');
    
    const testWords = ['ìì—°', 'ë‚˜ë¬´', 'ì‚°', 'ë°”ë‹¤', 'ì‚¬ëŒ', 'ìŒì‹', 'í•™êµ', 'ì‚¬ë‘', 'í–‰ë³µ'];
    for (const word of testWords) {
      const vector = await vectorDB.getWordVector(word);
      if (vector) {
        console.log(`âœ… ${word}: vector loaded (${vector.length} dimensions)`);
        
        // ë‹¤ë¥¸ ë‹¨ì–´ì™€ì˜ ìœ ì‚¬ë„ í…ŒìŠ¤íŠ¸
        if (word !== 'ìì—°') {
          const vector1 = await vectorDB.getWordVector('ìì—°');
          if (vector1) {
            const similarity = cosineSimilarity(vector1, vector);
            const score = Math.min(100, Math.max(0, (similarity + 0.15) * 120));
            console.log(`   ğŸ”— Similarity to 'ìì—°': ${score.toFixed(2)}`);
          }
        }
      } else {
        console.log(`âŒ ${word}: vector not found`);
      }
    }

    console.log('âœ… LARGE VectorDB initialization completed successfully!');
    console.log('ğŸ® You now have access to a massive Korean word database');
    console.log(`ğŸ“Š Database location: ${DB_PATH}`);
    console.log(`ğŸ“¦ Total vectors: ${insertedCount}`);
    console.log(`ğŸš€ Coverage: ${(insertedCount/targetWords.length*100).toFixed(1)}%`);

  } catch (error) {
    console.error('âŒ Error during initialization:', error);
    process.exit(1);
  } finally {
    await vectorDB.close();
  }
}

function cosineSimilarity(vec1, vec2) {
  if (vec1.length !== vec2.length) return 0;

  let dotProduct = 0;
  let norm1 = 0;
  let norm2 = 0;

  for (let i = 0; i < vec1.length; i++) {
    dotProduct += vec1[i] * vec2[i];
    norm1 += vec1[i] * vec1[i];
    norm2 += vec2[i] * vec2[i];
  }

  const magnitude = Math.sqrt(norm1) * Math.sqrt(norm2);
  if (magnitude === 0) return 0;

  return dotProduct / magnitude;
}

// CLI ì‹¤í–‰
initializeLargeDatabase().catch(console.error);
